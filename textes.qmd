---
title: "Données textuelles et non structurées"
---


La collecte et l'utilisation accrue
de données textuelles est l'un
des moteurs de la prolifération
de données émergentes.
L'utilisation accrue de services
sur le _web_ associée à des méthodes
pour collecter et traiter ces 
traces numériques particulières
a relancé l'usage de ces données pour en
faire, aujourd'hui, l'une des sources les 
plus prometteuses de la statistique publique
et l'un des champs les plus actifs de la recherche
en _data science_.
En effet, une partie des méthodes textuelles qui entrent 
dans la palette des compétences des _data-scientists_
spécialistes du traitement de données textuelles sont assez
anciennes. 
Par exemple, la distance de Levensthein a été proposée
pour la première fois en 1965, l'ancêtre des réseaux
de neurone actuels est le perceptron qui date de 1957...
Néanmoins, le fait que certaines entreprises du net
basent leur _business model_ sur le traitement
et la valorisation de la donnée
textuelle, notamment Google, Facebook et Twitter, a amené 
à renouveler le champ. 

La statistique publique s'appuie également sur la collecte
et le traitement de données textuelles. Les collectes
de données officielles ne demandent jamais exclusivement
des données numériques. Les premières informations
demandées sont généralement un état civil, une adresse...
Ensuite, en fonction du thème de l'enquête, d'autres
informations textuelles seront collectées: un nom
d'entreprise, un titre de profession... Les données
administratives elles-aussi présentent beaucoup d'informations
textuelles. Ces données défient l'analyse statistique car
cette dernière, qui vise à détecter des grandes structures
à partir d'observations multiples, doit s'adapter à la différence
des données textuelles: le langage est un champ où
certaines des notions usuelles de la statistique (distance, similarité...)
doivent être revues.

Ce chapitre propose un panorama très incomplet de l'apport
des données non structurées, principalement textuelles, 
pour la statistique et l'analyse de données. Nous évoquerons
plusieurs sources ou méthodes de collecte. Nous ferons
quelques détours par des exemples pour aller plus
loin.


réseaux sociaux, champs textuels (enquête, production automatisée, données privées), modèles de langage (GPT-3) et tchatbot 

# Webscraping


## Présentation


Le _webscraping_ est une méthode de collecte de données qui repose
sur le moissonnage d'objets à grande dimension (des pages web)
afin d'en extraire des informations ponctuelles (du texte, des nombres...).

Le [webscraping](https://fr.wikipedia.org/wiki/Web_scraping) désigne les techniques d'extraction du contenu des sites internet. C'est une pratique très utile pour toute personne souhaitant travailler sur des informations disponibles en ligne, mais n'existant pas forcément sous la forme d'un tableau *Excel*.

## Enjeux pour la statistique publique

Le *webscraping* a un certain nombre d'enjeux en termes de légalité, qui ne seront pas enseignés dans ce cours. En particulier, la CNIL a publié en 2020 de nouvelles directives sur le *webscraping* reprécisant que toute donnée ne peut être réutilisée à l'insu de la personne à laquelle ces données appartiennent.

Le *webscraping* est un domaine où la reproductibilité est compliquée à mettre en oeuvre. Une page *web* évolue
potentiellement régulièrement et d'une page web à l'autre, la structure peut
être très différente ce qui rend certains codes difficilement exportables.
Par conséquent, la meilleure manière d'avoir un programme fonctionnel est
de comprendre la structure d'une page web et dissocier les éléments exportables
à d'autres cas d'usages des requêtes *ad hoc*.

Un code qui fonctionne aujourd'hui peut ainsi très bien ne plus fonctionner
au bout de quelques semaines. Il apparaît
préférable de privilégier les API
qui sont un accès en apparence plus compliqué mais en fait plus fiable à moyen terme.
Cette difficulté à construire une extraction de données pérenne par
_webscraping_ une illustration du principe _"there is no free lunch"_.
La donnée est au cœur du business model de nombreux acteurs, il est donc logique qu'ils essaient de restreindre la moisson de leurs données. 

Les APIs sont un mode d'accès de plus en plus généralisé à des données.
Cela permet un lien direct entre fournisseurs et utilisateurs de données,
un peu sous la forme d'un contrat. Si les données sont ouvertes avec restrictions, on utilise des clés d'authentification.
Avec les API, on structure sa demande de données sous forme de requête paramétrée (source désirée, nombre de lignes, champs...) et le fournisseur de données y répond, généralement sous la forme d'un résultat au format JSON. 
Pour plus de détails, vous pouvez explorer le
[chapitre sur les API dans le cours de `Python` de l'ENSAE](https://pythonds.linogaliana.fr/api/).
On n'est pas à l'abri de mauvaise surprise avec les
APIs (indisponibilité, limite atteinte de requêtes...) mais cela permet un lien plus direct avec la dernière donnée publiée par un producteur et plus fiable que le webscraping.

## Exemples dans la statistique publique



## Implémentations

`Python` est le langage le plus utilisé
par les _scrappers_. 
`BeautifulSoup` sera suffisant quand vous voudrez travailler sur des pages HTML statiques. Dès que les informations que vous recherchez sont générées via l'exécution de scripts [Javascript](https://fr.wikipedia.org/wiki/JavaScript), il vous faudra passer par des outils comme [Selenium](https://selenium-python.readthedocs.io/).
De même, si vous ne connaissez pas l'URL, il faudra passer par un framework comme [Scrapy](https://scrapy.org/), qui passe facilement d'une page à une autre ("crawl"). Scrapy est plus complexe à manipuler que BeautifulSoup : si vous voulez plus de détails, rendez-vous sur la page du [tutoriel Scrapy](https://doc.scrapy.org/en/latest/intro/tutorial.html).
Pour plus de détails, voir le [TP sur le webscraping en 2e année de l'ENSAE](https://pythonds.linogaliana.fr/webscraping/)

Les utilisateurs de `R` privilégieront `rvest`

# Réseaux sociaux

Les réseaux sociaux sont l'une des sources textuelles
les plus 
communes. C'est leur usage à des fins commerciales
qui a amené les entreprises du net à renouveler
le champ de l'analyse textuelles qui bénéficie
au-delà de leur champ d'origine. 

Ces données sont notamment très utilisées dans le champ
de l'analyse de sentiment. 
Citer des recherches


# Les modèles de langage 

un modèle de langage est un modèle statistique qui modélise la distribution de séquences de mots, plus généralement de séquences de symboles discrets (lettres, phonèmes, mots), dans une langue naturelle. Un modèle de langage peut par exemple prédire le mot suivant une séquence de mots1.

BERT, GPT-3 et Bloom sont des modèles de langage. 

GPT-3 (acronyme de Generative Pre-trained Transformer 3) est un modèle de langage développé par la société OpenAI, annoncé le 28 mai 2020, ouvert aux utilisateurs via l'API d'OpenAI en juillet 2020. 
GPT-3 est le plus gros modèle de langage jamais entraîné avec 175 milliards de paramètres.

@brown2020language

champs textuels:
libellés: appariement et modélisation

NLP

# References

::: {#refs}
:::
