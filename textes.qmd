---
title: "Données textuelles et non structurées"
---


La collecte et l'utilisation accrue
de données textuelles est l'un
des moteurs de la prolifération
de données émergentes.
L'utilisation accrue de services
sur le _web_ associée à des méthodes
pour collecter et traiter ces 
traces numériques particulières
a relancé l'usage de ces données pour en
faire, aujourd'hui, l'une des sources les 
plus prometteuses de la statistique publique
et l'un des champs les plus actifs de la recherche
en _data science_.
En effet, une partie des méthodes textuelles qui entrent 
dans la palette des compétences des _data-scientists_
spécialistes du traitement de données textuelles sont assez
anciennes. 
Par exemple, la distance de Levensthein a été proposée
pour la première fois en 1965, l'ancêtre des réseaux
de neurone actuels est le perceptron qui date de 1957...
Néanmoins, le fait que certaines entreprises du net
basent leur _business model_ sur le traitement
et la valorisation de la donnée
textuelle, notamment Google, Facebook et Twitter, a amené 
à renouveler le champ. 

La statistique publique s'appuie également sur la collecte
et le traitement de données textuelles. Les collectes
de données officielles ne demandent jamais exclusivement
des données numériques. Les premières informations
demandées sont généralement un état civil, une adresse...
Ensuite, en fonction du thème de l'enquête, d'autres
informations textuelles seront collectées: un nom
d'entreprise, un titre de profession... Les données
administratives elles-aussi présentent beaucoup d'informations
textuelles. Ces données défient l'analyse statistique car
cette dernière, qui vise à détecter des grandes structures
à partir d'observations multiples, doit s'adapter à la différence
des données textuelles: le langage est un champ où
certaines des notions usuelles de la statistique (distance, similarité...)
doivent être revues.

Ce chapitre propose un panorama très incomplet de l'apport
des données non structurées, principalement textuelles, 
pour la statistique et l'analyse de données. Nous évoquerons
plusieurs sources ou méthodes de collecte. Nous ferons
quelques


webscraping, réseaux sociaux, champs textuels (enquête, production automatisée, données privées), modèles de langage (GPT-3) et tchatbot 

# Webscraping

Le _webscraping_ est une méthode de collecte de données qui repose
sur le moissonnage d'objets à grande dimension (des pages web)
afin d'en extraire des informations ponctuelles (du texte, des nombres...).


# Les modèles de langage 


GPT-3 (acronyme de Generative Pre-trained Transformer 3) est un modèle de langage développé par la société OpenAI, annoncé le 28 mai 2020, ouvert aux utilisateurs via l'API d'OpenAI en juillet 2020. 
GPT-3 est le plus gros modèle de langage jamais entraîné avec 175 milliards de paramètres.

@brown2020language

champs textuels:
libellés: appariement et modélisation

NLP

# References

::: {#refs}
:::
